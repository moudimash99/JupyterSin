{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.random.rand(1,256)\nX -= 0.5\nX *= math.pi\nY = np.sin(X)\nlayer_size = [1,20,5,2,1]\nlayers = len(layer_size)\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    z = 1/(1 + np.exp(-x))\n    return z","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_grad(x):\n    z = 1/(1 + np.exp(-x))\n    return (z) * (1-z)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_propagation(params,X,layer_size):\n    layers = len(layer_size)\n    cache = {\"A0\":X}\n    cA = X\n    L = layers-1\n    for i in range(1,layers-1):\n       \n        current_l = i\n        cW = params[\"W\"+str(current_l)]\n        cb = params[\"b\"+str(current_l)]\n        halfZ = np.dot(cW,cA)\n        cZ = halfZ + cb\n        cA = np.tanh(cZ)\n        cache[\"A\"+str(current_l)]= cA\n    cW = params[\"W\"+str(L)]\n    cb = params[\"b\"+str(L)]\n    cache[\"A\"+str(L)] = np.dot(cW,cA) + cb\n    return cache","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def intialize_params(layer_size):\n    layers = len(layer_size)\n    parms = {}\n    for i in range(1,layers):\n        current_l = i\n        parms[\"W\"+str(current_l)] = np.random.rand(layer_size[i],layer_size[i-1]) * 0.01\n        parms[\"b\"+str(current_l)] = np.zeros((layer_size[i],1))\n    return parms","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parm(grad,parms,alpha,layers):\n    for i in range(1,layers):\n        current_l = i\n        parms[\"W\"+str(current_l)] = parms[\"W\"+str(current_l)] + alpha * grad[\"dW\"+str(current_l)]\n#         print(parms[\"b\"+str(current_l)].shape)\n#         print(grad[\"db\"+str(current_l)].shape)\n#         print((grad[\"db\"+str(current_l)]+parms[\"b\"+str(current_l)]).shape)\n#         print(\"###########\")\n        \n        parms[\"b\"+str(current_l)] = parms[\"b\"+str(current_l)] + alpha * grad[\"db\"+str(current_l)]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def backpropag(cache,parms,layers,Y):\n    grad = {}\n    cA = cache[\"A\"+str(layers-1)]\n    #mid = np.multiply(cA,1-cA)\n    mid = 1\n    last = cache[\"A\"+str(layers-2)]\n    error = (Y - cA)\n    delta = np.multiply(error,mid)\n    \n    grad[\"dW\"+str(layers-1)] =  -np.dot(delta,last.T)\n    ds = - np.sum(delta,axis = 1)\n    grad[\"db\"+str(layers-1)] = ds.reshape(ds.shape[0],1)\n    for i in range(layers-2,0,-1):\n        cW = parms[\"W\"+str(i)]\n        cW2 = parms[\"W\"+str(i+1)]\n        cb = parms[\"b\"+str(i)]\n        cA = cache[\"A\"+str(i)]\n        error = np.dot(cW2.T,delta)\n        mid = np.multiply(cA,1-cA)\n        last = cache[\"A\"+str(i-1)]\n        delta = np.multiply(error,mid)\n        grad[\"dW\"+str(i)] = - np.dot(delta,last.T)\n        ds = - np.sum(delta,axis = 1)\n        grad[\"db\"+str(i)] = ds.reshape(ds.shape[0],1)\n    \n        \n    \n    return grad","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(a3,Y):\n    m = Y.shape[1]\n    dif = a3 - Y\n    dif = np.multiply(dif,dif)\n    dif = np.sum(dif,axis = 1)\n    \n    return dif / (  m)\n    \n    \n    \n    ","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = intialize_params(layer_size)\nh = forward_propagation(p,X,layer_size)\ns = backpropag(h,p,layers,Y)\nupdate_parm(s,p,0.1,layers)\ndW4 = s[\"dW4\"]\n\n# for d in s:\n#     print(d)\n#     print(s[d].shape)\n#     print(\"\\n#####\\n\")    \n\n# for d in p:\n#     print(d)\n#     print(p[d].shape)\n#     print(\"\\n#####\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y= np.array([[1,0,0,1,1,2,1]])\na3 = np.array([[0.7,0.5,0.5,2,1,1.8,1]])\nprint(loss(a3,Y))","execution_count":21,"outputs":[{"output_type":"stream","text":"[0.11642857]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = intialize_params(layer_size)\nX = np.random.rand(1,256)\nY = 2 * X\nfor i in range(20):\n    print(i)\n    h = forward_propagation(p,X,layer_size)\n    s = backpropag(h,p,layers,Y)\n    update_parm(s,p,0.1,layers)\n    if(i % 1 == 0):\n        losss = loss(h[\"A6\"],Y)\n        print(\"loss is : \"+ str(losss))\n        \n    \n    \n\n","execution_count":24,"outputs":[{"output_type":"stream","text":"0\nloss is : [1.32897674]\n1\nloss is : [718.87762392]\n2\nloss is : [508427.59931786]\n3\nloss is : [3.59742675e+08]\n4\nloss is : [2.50783684e+11]\n5\nloss is : [5.99402095e+15]\n6\nloss is : [1.59128383e+19]\n7\nloss is : [3.80335293e+23]\n8\nloss is : [9.09045469e+27]\n9\nloss is : [2.17272412e+32]\n10\nloss is : [5.19306269e+36]\n11\nloss is : [1.24120222e+41]\n12\nloss is : [2.96661729e+45]\n13\nloss is : [7.09055945e+49]\n14\nloss is : [1.69472596e+54]\n15\nloss is : [4.05059163e+58]\n16\nloss is : [9.68138386e+62]\n17\nloss is : [2.31396305e+67]\n18\nloss is : [5.5306401e+71]\n19\nloss is : [1.32188714e+76]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}